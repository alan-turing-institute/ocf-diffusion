{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03c4d82-667d-4a4c-b842-14bf5be8734a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import random, os\n",
    "\n",
    "from cloudcasting.dataset import SatelliteDataset\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.mps.manual_seed(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd74f65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nsimpson/Code/ocf-diffusion/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from cloudcasting.constants import NUM_CHANNELS\n",
    "\n",
    "\n",
    "class ConditionedUnet(nn.Module):\n",
    "    history_steps: int\n",
    "    def __init__(self, image_size, history_steps = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.history_steps = history_steps\n",
    "\n",
    "        # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (previous timesteps)\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=image_size,  # the target image resolution\n",
    "            in_channels=NUM_CHANNELS + history_steps * NUM_CHANNELS,  # noise input + conditioning information\n",
    "            out_channels=NUM_CHANNELS,  # the number of output channels\n",
    "            layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "            block_out_channels=(\n",
    "                128, \n",
    "                256, \n",
    "                # 512,\n",
    "            ),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "                \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "                # \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "\n",
    "                # \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "                # \"AttnDownBlock2D\",\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                # \"AttnUpBlock2D\",\n",
    "                # \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "                \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "                \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "                # \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            ),\n",
    "            add_attention = False,  # blows up memory with attention -- maybe do latent diffusion\n",
    "        )\n",
    "\n",
    "    # Our forward method now takes the class labels as an additional argument\n",
    "    def forward(self, noisy_image, conditioning, t):\n",
    "\n",
    "        # stack noisy image and conditioning info along the time axis\n",
    "        stacked = torch.cat([noisy_image, conditioning], dim=-3) \n",
    "        print(f\"{stacked.shape=}\")\n",
    "\n",
    "        # reshape to (batch, channels*time, height, width)\n",
    "        net_input = stacked.reshape(-1, stacked.shape[-4] * stacked.shape[-3], *stacked.shape[-2:])\n",
    "\n",
    "        return self.model(net_input, t).sample\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b73538-de30-4cdd-8312-c7529e7795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudcasting.constants import DATA_INTERVAL_SPACING_MINUTES\n",
    "\n",
    "TRAINING_DATA_PATH = \"/Users/nsimpson/code/climetrend/cloudcast/2020_training_nonhrv.zarr\"\n",
    "HISTORY_STEPS = 1\n",
    "\n",
    "# Instantiate the torch dataset object\n",
    "dataset = SatelliteDataset(\n",
    "    zarr_path=TRAINING_DATA_PATH,\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    history_mins=(HISTORY_STEPS - 1) * DATA_INTERVAL_SPACING_MINUTES,\n",
    "    forecast_mins=15,\n",
    "    sample_freq_mins=15,\n",
    "    nan_to_num=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "840df044-25d6-4010-ba62-1c27485ed939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2868\n",
      "NaN percentage: <xarray.Dataset> Size: 8B\n",
      "Dimensions:  ()\n",
      "Data variables:\n",
      "    data     float64 8B 0.04845\n"
     ]
    }
   ],
   "source": [
    "# Number of complete samples in the dataset\n",
    "# - this includes overlapping periods, not completely distinct periods\n",
    "n_samples = len(dataset)\n",
    "print(n_samples)\n",
    "\n",
    "# nan percentage\n",
    "print(f\"NaN percentage: {np.mean(np.isnan(dataset.ds)).compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fa56fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     np.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(0)\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset=dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=num_workers,\n",
    "#     worker_init_fn=seed_worker,\n",
    "#     generator=g,\n",
    "# )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e8c447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_dataloader))\n",
    "\n",
    "assert X.shape == y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e414a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# Calculate required crop for the input dims to be divisible by 16\n",
    "x_cropped_shape = [(size // 16) * 16 for size in X.shape[-2:]]\n",
    "model = ConditionedUnet(x_cropped_shape, history_steps=HISTORY_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353a90bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1434 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked.shape=torch.Size([2, 11, 2, 368, 608])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1434 [00:12<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.155314\n",
      "Finished epoch 0. Average of the last 100 loss values: 0.011553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x153061550>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Create a scheduler\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\")\n",
    "\n",
    "model = model.to(device) # model on device\n",
    "\n",
    "# How many runs through the data should we do?\n",
    "n_epochs = 1\n",
    "\n",
    "# Our loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# The optimizer\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Keeping a record of the losses for later viewing\n",
    "losses = []\n",
    "\n",
    "\n",
    "def crop(x):\n",
    "    # Calculate required crop for the input dims to be divisible by 16\n",
    "    x_cropped_shape = [(size // 16) * 16 for size in x.shape[-2:]]\n",
    "    return x[..., :x_cropped_shape[0], :x_cropped_shape[1]]\n",
    "\n",
    "# The training loop\n",
    "for epoch in range(n_epochs):\n",
    "    for i, (x, y) in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "        # Get some data and prepare the corrupted version\n",
    "        x = crop(x.to(device) * 2 - 1)  # Data on the GPU (mapped to (-1, 1)) (????)\n",
    "        \n",
    "        y = crop(y.to(device) * 2 - 1)\n",
    "        # Calculate required crop for the input dims to be divisible by 16\n",
    "        y_cropped_shape = [(size // 16) * 16 for size in y.shape[-2:]]\n",
    "        y_cropped = y[..., :y_cropped_shape[0], :y_cropped_shape[1]]\n",
    "        noise = torch.randn_like(y)  # base noise on the target image\n",
    "        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)\n",
    "        noisy_y = noise_scheduler.add_noise(y, noise, timesteps)  # Note that we pass in the target y\n",
    "\n",
    "        # Predict the noise based on previous images and the noisy forecast target\n",
    "        pred = model(noisy_y, x, timesteps).reshape(y.shape)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(pred, noise)  # How close is the output to the noise\n",
    "\n",
    "        # Backprop and update the params:\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Store the loss for later\n",
    "        losses.append(loss.item())\n",
    "        print(f\"Loss: {loss.item():05f}\")\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            break\n",
    "\n",
    "    # Print out the average of the last 100 loss values to get an idea of progress:\n",
    "    avg_loss = sum(losses[-100:]) / 100\n",
    "    print(f\"Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}\")\n",
    "\n",
    "# View the loss curve\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50d95ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.1553138494491577]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e6217a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pvnet0",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
