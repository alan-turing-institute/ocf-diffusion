{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03c4d82-667d-4a4c-b842-14bf5be8734a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import random, os\n",
    "\n",
    "from cloudcasting.dataset import SatelliteDataset\n",
    "\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.mps.manual_seed(seed)\n",
    "    \n",
    "seed_everything(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd74f65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler, UNet2DModel\n",
    "from cloudcasting.constants import NUM_CHANNELS, IMAGE_SIZE_TUPLE, NUM_FORECAST_STEPS\n",
    "\n",
    "\n",
    "class ConditionedUnet(nn.Module):\n",
    "    history_steps: int\n",
    "    def __init__(self, image_size, history_steps = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.history_steps = history_steps\n",
    "\n",
    "        # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (previous timesteps)\n",
    "        self.model = UNet2DModel(\n",
    "            sample_size=image_size,  # the target image resolution\n",
    "            in_channels=NUM_CHANNELS + (history_steps - 1) * NUM_CHANNELS,  # Additional input channels for previous timesteps\n",
    "            out_channels=NUM_CHANNELS,  # the number of output channels\n",
    "            layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "            block_out_channels=(128, 256, 512),\n",
    "            down_block_types=(\n",
    "                \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "                \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "                \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "\n",
    "                # \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "                # \"AttnDownBlock2D\",\n",
    "            ),\n",
    "            up_block_types=(\n",
    "                # \"AttnUpBlock2D\",\n",
    "                # \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "                \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "                \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "                \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "            ),\n",
    "            add_attention = False,\n",
    "        )\n",
    "\n",
    "    # Our forward method now takes the class labels as an additional argument\n",
    "    def forward(self, x, t, rollout_steps = NUM_FORECAST_STEPS):\n",
    "        \n",
    "        sample_shape = x.shape[-4:] \n",
    "        print(f\"{sample_shape=}\")\n",
    "        assert sample_shape == (NUM_CHANNELS, self.history_steps, *IMAGE_SIZE_TUPLE), f\"Input shape {x.shape} does not match expected shape (channels, history_steps, height, width)\"\n",
    "\n",
    "        # Calculate required crop for the input dims to be divisible by 16\n",
    "        x_cropped_shape = [(size // 16) * 16 for size in x.shape[-2:]]\n",
    "        x_cropped = x[..., :x_cropped_shape[0], :x_cropped_shape[1]]\n",
    "        print(f\"{x_cropped.shape=}\")\n",
    "        \n",
    "        # Reshape input images (batch, channels, time, height, width) to (batch, channels*time, height, width)\n",
    "        # use negative indexing to allow for non-batched input\n",
    "        net_input = x_cropped.reshape(-1, x_cropped.shape[-4] * x_cropped.shape[-3], *x_cropped.shape[-2:])\n",
    "\n",
    "        return self.model(net_input, t).sample\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91b73538-de30-4cdd-8312-c7529e7795d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cloudcasting.constants import DATA_INTERVAL_SPACING_MINUTES\n",
    "\n",
    "TRAINING_DATA_PATH = \"/Users/nsimpson/code/climetrend/cloudcast/2020_training_nonhrv.zarr\"\n",
    "HISTORY_STEPS = 1\n",
    "\n",
    "# Instantiate the torch dataset object\n",
    "dataset = SatelliteDataset(\n",
    "    zarr_path=TRAINING_DATA_PATH,\n",
    "    start_time=None,\n",
    "    end_time=None,\n",
    "    history_mins=(HISTORY_STEPS - 1) * DATA_INTERVAL_SPACING_MINUTES,\n",
    "    forecast_mins=180,\n",
    "    sample_freq_mins=15,\n",
    "    nan_to_num=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840df044-25d6-4010-ba62-1c27485ed939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of complete samples in the dataset\n",
    "# - this includes overlapping periods, not completely distinct periods\n",
    "n_samples = len(dataset)\n",
    "print(n_samples)\n",
    "\n",
    "# nan percentage\n",
    "print(f\"NaN percentage: {np.mean(np.isnan(dataset.ds)).compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fa56fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     np.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(0)\n",
    "\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "# dataloader = DataLoader(\n",
    "#     dataset=dataset,\n",
    "#     batch_size=batch_size,\n",
    "#     num_workers=num_workers,\n",
    "#     worker_init_fn=seed_worker,\n",
    "#     generator=g,\n",
    "# )\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8c447f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(dataloader))\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e414a95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "# Calculate required crop for the input dims to be divisible by 16\n",
    "x_cropped_shape = [(size // 16) * 16 for size in X.shape[-2:]]\n",
    "model = ConditionedUnet(x_cropped_shape, history_steps=HISTORY_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3eea96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.to(\"mps\")\n",
    "model = model.to(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4072ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353a90bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "pvnet0",
   "name": "pytorch-gpu.1-13.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-13:m103"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
